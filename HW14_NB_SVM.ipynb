{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes and SVC \n",
    "\n",
    "**A note on this document**\n",
    "This document is known as a Jupyter notebook; it allows text and executable code to coexist in a very easy-to-read format. Blocks can contain text or executable code. For blocks containing code, press `Shift + Enter`, `Ctrl+Enter`, or click the arrow on the block to run the code. Earlier blocks of code need to be run for the later blocks of code to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris Flowers\n",
    "\n",
    "In classification problems, the output space consists of a set of $C$ labels, which are referred to as `classes`. This set is denoted as $\\mathcal{Y} = \\{1, 2, \\ldots, C\\}$. The task of predicting the class label based on an input is commonly known as `pattern recognition`. When there are only two classes, they are often represented as $y \\in \\{0, 1\\}$ or $y \\in \\{-1, +1\\}$, and this specific scenario is called `binary classification`.\n",
    "\n",
    "For instance, let's consider the task of classifying Iris flowers into their three subspecies: Setosa, Versicolor, and Virginica. The figure below showcases an example from each of these classes.\n",
    "\n",
    "<div>\n",
    "<img src=\"./figures/iris.png\" width=\"600\"/>\n",
    "</div>\n",
    "Three types of Iris flowers: Setosa (L), Versicolor (C), and Virginica (R). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features of the Iris dataset are: sepal length, sepla widht, petal length, petal width.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "print(iris.feature_names)\n",
    "print(iris.target_names)\n",
    "## Iris Flowers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Iris dataset is a collection of 150 labeled examples of Iris flowers, 50 of each type, described by these 4 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Convert to pandas dataframe\n",
    "df = pd.DataFrame(data=X, columns=iris.feature_names)\n",
    "df[\"label\"] = pd.Series(iris.target_names[y], dtype=\"category\")\n",
    "\n",
    "df.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For tabular data with a small number of features, it is common to make a `pair plot`, in which panel $(i, j)$ shows a scatter plot of variables $i$ and $j$, and the diagonal entries $(i,i)$ show the marginal density of variable $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# we pick a color map to match that used by decision tree graphviz\n",
    "palette = {\"setosa\": \"orange\", \"versicolor\": \"green\", \"virginica\": \"purple\"}\n",
    "\n",
    "g = sns.pairplot(df, vars=df.columns[0:4], hue=\"label\", palette=palette)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above clearly shows that Iris setosa can be readily classified, whereas distinguishing between Iris versicolor and Iris virginica is more challenging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the `train_test_split` function in scikit-learn to split the dataset into a **training set** and a **testing (or validation) set**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier\n",
    "\n",
    "Let's try the Naive Bayes method to classify the Iris flowers. We will use the `Guassian` Naive Bayes because the dataset is continuous, real-valued features where the values are assumed to be normally distributed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Naive Bayes classifier\n",
    "nb_classifier = GaussianNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to use 5-fold cross-validation to find accuracy scores and their average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Naive Bayes classifier (Gaussian Naive Bayes for this example)\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "cv_scores = cross_val_score(nb_classifier, X, y, cv=5)\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "\n",
    "# Calculate and print the mean accuracy\n",
    "mean_accuracy = np.mean(cv_scores)\n",
    "print(\"Mean accuracy:\", mean_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM) Classifier\n",
    "\n",
    "Let's try the Naive Bayes method to classify the Iris flowers. \n",
    "\n",
    "Kernels in SVMs are mathematical functions that are used to transform the original feature space into a higher-dimensional space. SVMs are primarily used for binary classification in their simplest form, and kernels help SVMs handle non-linearly separable data. There are several types of kernels we can use in SVMs:\n",
    "\n",
    "- Linear Kernel (default): It is suitable for linearly separable data and assumes that the data can be separated by a straight line (hyperplane).\n",
    "- Polynomial Kernel: This kernel is useful when data is not linearly separable and can be separated by a polynomial boundary.\n",
    "- Radial Basis Function (RBF) Kernel: The RBF kernel is often a good choice when the decision boundary is not expected to be linear or polynomial. It is a versatile kernel for handling non-linear data.\n",
    "- Sigmoid Kernel: The sigmoid kernel can be used for data that follows sigmoid-like patterns.\n",
    "\n",
    "Note that we did not discuss the RBF and sigmoid kernels in class.  \n",
    "\n",
    "The choice of kernel depends on the nature of your data and the problem we are trying to solve. It's often a good idea to experiment with different kernels to find the one that works best for your specific dataset.\n",
    "\n",
    "We will use the linear kernel for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train an SVM classifier\n",
    "svm_classifier = SVC(kernel=\"linear\")\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to use 5-fold cross-validation to find the accuracy and their average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "cv_scores = cross_val_score(svm_classifier, X, y, cv=5)\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "\n",
    "# Calculate and print the mean accuracy\n",
    "mean_accuracy = np.mean(cv_scores)\n",
    "print(\"Mean accuracy:\", mean_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST-784\n",
    "\n",
    "MNIST-784 is a widely used dataset in the field of machine learning and computer vision. It stands for the \"Modified National Institute of Standards and Technology\" database. The MNIST dataset contains a large collection of handwritten digits (0 through 9), which are commonly used for training and testing machine learning models, especially for image classification tasks. Each image in the MNIST dataset is a grayscale image with a resolution of 28x28 pixels, resulting in 784 total pixels. These images are typically used to develop and test algorithms for digit recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the MNIST dataset\n",
    "\n",
    "mnist_filename = \"data/mnist_784.pkl\"\n",
    "path = Path(mnist_filename)\n",
    "\n",
    "if not path.is_file():\n",
    "    # download the dataset. It will take about a minute.\n",
    "    mnist_dataset = datasets.fetch_openml(\"mnist_784\")\n",
    "    joblib.dump(mnist_dataset, mnist_filename)\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist = joblib.load(mnist_filename)\n",
    "\n",
    "# Split the data into features and labels\n",
    "features = mnist.data\n",
    "labels = mnist.target\n",
    "\n",
    "# features is a pandas.core.frame.DataFrame object\n",
    "print(features.info())\n",
    "\n",
    "# labels is a pandas.core.series.Series object\n",
    "print(labels.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As indicated in the above information, the dataset comprises 70,000 entries. Let's display the first few entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reshape and display the first few images\n",
    "rows = 5  # Change this value to display more images\n",
    "cols = 10  # Change this value to display more images\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        data = features.iloc[i * cols + j]\n",
    "        image = data.to_numpy().reshape(28, 28)\n",
    "        plt.subplot(rows, cols, i * cols + j + 1)\n",
    "        plt.imshow(image, cmap=\"gray\")\n",
    "        plt.title(f\"{labels.iloc[i*cols+j]}\")\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features is a pandas.core.frame.DataFrame object\n",
    "print(features.shape)\n",
    "\n",
    "print(features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverable 1\n",
    "\n",
    "Use the Naive Bayes method to classify the handwritten digits. \n",
    "\n",
    "The very first step is to normalize the pixel values to the range [0, 1]. For the MNIST dataset, which consists of grayscale pixel values ranging from 0 to 255, normalizing by dividing all pixel values by 255 is a straightforward way to achieve this. This normalization ensures that each pixel value is between 0 and 1, making the dataset more suitable for training neural networks and other machine learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "features = mnist.data\n",
    "labels = mnist.target\n",
    "\n",
    "# Preprocess the data\n",
    "features_normalized = features / 255.0  # Normalize the pixel values to the range [0, 1]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features_normalized, labels, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `Multinomial` Naive Bayes because the dataset is discrete pixel values, where the features are generated from a multinomial distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Write your code for multinomial Naive Bayes that classifies the handwritten digits.\n",
    "# Ensure you print out the accuracy score.\n",
    "\n",
    "# Create a Multinomial Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following index of misclassification to display the misclassified digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mis_indices_nb = np.where(y_pred != y_test.values)[0]  # index of misclassification\n",
    "\n",
    "# Write your code to display the first 10 misclassified digits.\n",
    "# You should add the predicted values on top of the digits.\n",
    "# To do this, use plt.title."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use 5-fold cross-validation to find the accuracy scores and their average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverable 2\n",
    "\n",
    "Use the SVM method to classify the handwritten digits. \n",
    "\n",
    "We will use the `linear` kernel to classify this dataset.  The linear kernel is suitable for linearly separable data and assumes that the data can be separated by a straight line (hyperplane).\n",
    "\n",
    "Warning: It will take about 3-7 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code for SVM with linear kernel that classifies the handwritten digits.\n",
    "# Ensure you print out the accuracy score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's experiment with the `Radial Basis Function (RBF) kernel`, which is often a good choice when the decision boundary is not expected to be linear or polynomial. It is a versatile kernel for handling non-linear data.\n",
    "\n",
    "Warning: It will take about 3-7 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code for SVM with RBF kernel that classifies the handwritten digits.\n",
    "# Ensure you print out the accuracy score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm whether the digits misclassified by the Naive Bayes classifier are now correctly classified by SVM. Display these misclassified digits along with their predicted values from SVM. To do this, use the same `mis_indices_nb` in conjunction with the `y_pred` from the SVM classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code to display the first 10 misclassified digits by Naive Bayes.\n",
    "# You should add the predicted values by SVM on top of the digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use 5-fold cross-validation to find the accuracy scores and their average.\n",
    "\n",
    "Warning: It will take about 20 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
